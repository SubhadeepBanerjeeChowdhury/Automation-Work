from fuzzywuzzy import fuzz

sheet_names = ["BLT Mgmt", "ChelseaGroton", "Chelsea Groton Bank", "Aurora P"]
groups = ["Aurora Productions INC", "BLT Management Inc", 'Chelsea Groton Bank Inc', 'Aurroras Productions']

# Define the fuzzy matching methods to be used
matching_methods = ['partial_ratio', 'token_set_ratio', 'ratio', 'token_sort_ratio', 'WRatio', 'partial_token_sort_ratio']

# Initialize a dictionary to store the best match for each sheet name and its highest similarity score across all methods
best_matches = {}

# Iterate through each sheet name
for sheet_name in sheet_names:
    # Initialize variables to store the best match and its highest similarity score
    best_match = None
    best_score = 0
    
    # Iterate through each group to find the best match using all matching methods
    for group in groups:
        # Initialize variable to store the highest score among all methods for the current group
        group_best_score = 0
        
        # Iterate through each matching method
        for method in matching_methods:
            # Calculate the similarity score between the sheet name and group using the current method
            score = getattr(fuzz, method)(sheet_name.lower(), group.lower())
            
            # Update the highest score for the current group if the current score is higher
            group_best_score = max(group_best_score, score)
        
        # Update best match and its score if the current group's highest score is higher than the overall best score
        if group_best_score > best_score:
            best_match = group
            best_score = group_best_score
    
    # Save the best match and its highest score in the dictionary
    best_matches[sheet_name] = {'group': best_match, 'score': best_score}

# Print the results
for sheet_name, match_info in best_matches.items():
    print(f"Sheet name: {sheet_name}, Best match: {match_info['group']}, Highest similarity score: {match_info['score']}")



******************
07/04/2024

MNS ID EXTRACTION:

import re

text = 'ADP MNS21-25, MNS 67-69 & 52, MNS 54-58, MNS44-49'

# Extract the range
mns_values = []
range_matches = re.findall(r'(?:MNS\s*)?(\d+)-(\d+)|\b(\d+)\b', text)

if range_matches:
    for match in range_matches:
        if match[0] and match[1]:  # Range pattern matched
            start_num = int(match[0])
            end_num = int(match[1])
            mns_values.extend([f'MNS{str(i).zfill(7)}' for i in range(start_num, end_num + 1)])
        elif match[2]:  # Individual number matched
            single_num = int(match[2])
            mns_values.append(f'MNS{str(single_num).zfill(7)}')

if mns_values:
    print(mns_values)
else:
    print("No match found.")


*******************************************************
08/04/2024


import pandas as pd

def process_dataframe(dataframe, date_or_group):
    # Assuming 'MNS FILTER' and 'CONTRACT OPT EFF DATE' are column names in the DataFrame
    MNS_FILTER = 'MNS FILTER'
    CONTRACT_OPT_EFF_DATE = 'CONTRACT OPT EFF DATE'
    PLAN_ID = 'PLAN ID'
    
    # Filtering DataFrame where MNS FILTER is 'MNS'
    df_filtered = dataframe[dataframe[MNS_FILTER] == 'MNS']
    
    # Filtering DataFrame where 'CONTRACT OPT EFF DATE' matches the user given date or group name
    if isinstance(date_or_group, str):
        df_filtered = df_filtered[df_filtered[CONTRACT_OPT_EFF_DATE] == date_or_group]
    elif isinstance(date_or_group, pd.Timestamp):  # If date is provided as pandas Timestamp
        df_filtered = df_filtered[df_filtered[CONTRACT_OPT_EFF_DATE] == date_or_group.strftime('%Y-%m-%d')]
    else:
        raise ValueError("Invalid input type. Please provide either a date (string or Timestamp) or a group name (string).")
    
    # Making a list of all elements in the 'PLAN ID' column
    plan_ids = df_filtered[PLAN_ID].tolist()
    
    return plan_ids

# Example usage:
# Assuming 'path' is your data and 'date_or_group' is either a date string or group name
# df = pd.DataFrame(path)
# result = process_dataframe(df, '1998-12-01')  # Example date
# result = process_dataframe(df, 'Group Name')  # Example group name


*************************************************************************************


# Convert to grayscale
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# Noise Reduction
blurred = cv2.GaussianBlur(gray, (5, 5), 0)

# Thresholding
_, thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

# Dilation and Erosion
kernel = np.ones((3, 3), np.uint8)
thresh = cv2.dilate(thresh, kernel, iterations=1)
thresh = cv2.erode(thresh, kernel, iterations=1)
***************************************************************************************




def key_correction(input_string):
    # Define the replacements for each pattern
    replacements = [
        'Family Deductible:',
        'Single Deductible:',
        'Family M.O.O.P.',
        'Single M.O.O.P.'
    ]

    # Define the regex patterns for each case
    patterns = [
        r'Family De.*?(\s|$)',
        r'Single De.*?(\s|$)',
        r'Family M.*?(\s|$)',
        r'Single M.*?(\s|$)'
    ]

    # Replace occurrences of each pattern with the corresponding replacement string
    for pattern, replacement in zip(patterns, replacements):
        input_string = re.sub(pattern, replacement, input_string, flags=re.IGNORECASE)

    return input_string


**************************************
img = cv2.imread(r'E:\Downloads\i0RDA.jpg')
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# Remove horizontal lines
thresh = cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY_INV,81,17)
horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (25,1))

# Using morph close to get lines outside the drawing
remove_horizontal = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, horizontal_kernel, iterations=3)
cnts = cv2.findContours(remove_horizontal, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
cnts = cnts[0] if len(cnts) == 2 else cnts[1]
mask = np.zeros(gray.shape, np.uint8)
for c in cnts:
    cv2.drawContours(mask, [c], -1, (255,255,255),2)

# First inpaint
img_dst = cv2.inpaint(img, mask, 3, cv2.INPAINT_TELEA)


********************************


# Read the image

gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# Remove horizontal lines
thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 81, 17)
horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (25, 1))
remove_horizontal = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, horizontal_kernel, iterations=3)

# Find contours of the horizontal lines
cnts = cv2.findContours(remove_horizontal, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
cnts = cnts[0] if len(cnts) == 2 else cnts[1]
mask = np.zeros(gray.shape, np.uint8)
for c in cnts:
    cv2.drawContours(mask, [c], -1, (255, 255, 255), 2)

# Dilate the mask to include text areas
mask = cv2.dilate(mask, np.ones((3, 3), np.uint8), iterations=2)

# Inpaint the image, excluding text areas
image = cv2.inpaint(img, mask, 3, cv2.INPAINT_TELEA)

********************************

def modified_cost_share(expression):
    expression=re.sub('[a-zA-Z]', '', expression)
    # Define a regex pattern to match currency amounts
    pattern = r'\$?(\d+(?:,\d+)*(?:\.\d{2})?)'

    # Find all matches in the expression
    matches = re.findall(pattern, expression)

    # Extract the second value if it exists
    if len(matches) >= 2:
        return float(matches[1].replace(',', ''))  # Convert to float and remove commas

    if len(matches) < 2:
        first_comma_index = expression.find(',')
        second_comma_index = expression.find(',', first_comma_index + 1)

        if second_comma_index != -1:
            return expression[second_comma_index- 1:]
        else:
            return expression
        

# Test expressions
expressions = [
    "$6,000 $4,000",
    "$6,000$4,000",
    "$6,000 4,000",
    "6000$4000_",
    "6,000 400000",
    "6,0005,500"
]

for expression in expressions:
    second_value = extract_second_value(expression)
    print(f"Expression: {expression}, Second Value: {second_value}")




********************************************************


import numpy as np
import pandas as pd

data = {'PCP/Specialist OV Copay': [np.nan,'$10/20', np.nan]}
df = pd.DataFrame(data).astype(str)

# Check each value for '/'
if df['PCP/Specialist OV Copay'].str.contains('/').any():
    # Convert the column values to strings
    df['PCP/Specialist OV Copay'] = df['PCP/Specialist OV Copay'].astype(str)

    # Split the column into two based on '/'
    df[['PCP COPAY', 'SPEC COPAY']] = df['PCP/Specialist OV Copay'].str.split('/', n=1, expand=True)


else:
    # Duplicate the values in both new columns
    df['PCP COPAY'] = df['PCP/Specialist OV Copay']
    df['SPEC COPAY'] = df['PCP/Specialist OV Copay']

# Replace NaNs with np.nan
df = df.replace({pd.np.nan: np.nan})

# Display the resulting DataFrame
df

*********************************

06/05/2024

def process_string(input_string):
    # Check if the pattern '_20(\d{2})_' is present in the input string
    if re.search(r'_20(\d{2})_', input_string):
        # Find the substring starting from 'ox' to the end
        match = re.search(r'ox(.*)', input_string, re.IGNORECASE)
        if match:
            result = match.group(0)  # Extract the matched substring including 'ox'
            uid = re.sub(r'_20\d{2}_', '', result)  # Remove '_20XX_' pattern
            uid = re.sub(r'\W+', '', uid.lower())
            uid = re.sub(r'_', '', uid.lower())  # Remove non-alphanumeric characters
            year_match = re.search(r'_20(\d{2})_', result)  # Extract year part
            year = '20'+year_match.group(1) if year_match else None
            return uid, year
    return input_string, np.nan

# Example DataFram
# Apply process_string to 'PLAN NAME' column
audit_df[['UID', 'YEAR']] = audit_df['CIRRUS TRACKING ID'].apply(lambda x: pd.Series(process_string(x)))

# Print the DataFrame
audit_df



def mapping_with_active_report(audit_df,tkinter_df):
    
    tkinter_df[['UID', 'YEAR']] = pd.DataFrame(tkinter_df['TRACKING ID'].astype(str).apply(process_string).tolist())
    merged1=pd.merge(tkinter_df,audit_df[['CIRRUS TRACKING ID','CIRRUS MNS ID','UID']],how='left',on='UID')
    merged2 = pd.merge(tkinter_df,audit_df[['CIRRUS TRACKING ID','CIRRUS MNS ID']], how='left', left_on='MNS ID1', right_on='CIRRUS MNS ID')
    merged3 = pd.merge(tkinter_df,audit_df[['CIRRUS TRACKING ID','CIRRUS MNS ID']], how='left', left_on='MNS ID2', right_on='CIRRUS MNS ID')
    df_fin=pd.concat([merged1,merged2,merged3], ignore_index=True)
    df_fin = df_fin.replace([None, 'nan'], np.nan)
    df_fin=df_fin.dropna(subset=['CIRRUS TRACKING ID', 'CIRRUS MNS ID'], how='all')
    df_fin=df_fin.drop_duplicates()
    df_sorted = df_fin.sort_values(by=['CIRRUS GROUP NAME', 'YEAR'],ascending=[False,False])
    return df_sorted
    




***************************************************************************************************



import openpyxl
import pandas as pd

# Load the workbook
wb = openpyxl.load_workbook('your_excel_file.xlsx')

# Function to check if a cell has strikethrough
def has_strikethrough(cell):
    if cell.font and cell.font.strike:
        return True
    return False

# Dictionary to store DataFrames for each sheet
sheets_data = {}

# Process each sheet in the workbook
for sheet_name in wb.sheetnames:
    sheet = wb[sheet_name]
    
    # Extract data while omitting strikethrough values
    data = []
    for row in sheet.iter_rows(values_only=False):  # Set values_only to False to access cell objects
        row_data = []
        for cell in row:
            if not has_strikethrough(cell):
                row_data.append(cell.value)
            else:
                row_data.append(None)  # or any placeholder you prefer
        data.append(row_data)
    
    # Create DataFrame
    df = pd.DataFrame(data)
    
    # Optionally, remove rows/columns that are fully None
    df.dropna(how='all', inplace=True)
    df.dropna(axis=1, how='all', inplace=True)
    
    # Store the DataFrame in the dictionary
    sheets_data[sheet_name] = df

# Display the DataFrames for each sheet
for sheet_name, df in sheets_data.items():
    print(f"Sheet name: {sheet_name}")
    print(df)
    print("\n")

# Optionally, save each DataFrame to a separate Excel file
with pd.ExcelWriter('filtered_excel_file.xlsx') as writer:
    for sheet_name, df in sheets_data.items():
        df.to_excel(writer, sheet_name=sheet_name, index=False)

